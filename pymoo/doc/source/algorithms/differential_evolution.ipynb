{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _nb_de:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Differential Evolution\n",
    "\n",
    "The classical single-objective\n",
    "differential evolution algorithm <cite data-cite=\"de\"></cite> where different crossover variations\n",
    "and methods can be defined. It is known for its good results for\n",
    "effective global optimization.\n",
    "\n",
    "The differential evolution crossover is simply defined by:\n",
    "\n",
    "$$\n",
    "v = x_{\\pi_1} + F \\cdot (x_{\\pi_2} - x_{\\pi_3})\n",
    "$$\n",
    "\n",
    "where $\\pi$ is a random permutation with with 3 entries. The difference is taken between individual 2 and 3 and added to the first one. This is shown below:\n",
    "\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 50%;\">\n",
    "![de_crossover](../resources/images/de_crossover.png)\n",
    "</div>\n",
    "\n",
    "\n",
    "Then, a second crossover between an individual and the so called donor vector $v$ is performed. The second crossover can be simply binomial/uniform or exponential.\n",
    "\n",
    "\n",
    "\n",
    "A great tutorial and more detailed information can be found [here](http://www1.icsi.berkeley.edu/~storn/code.html). The following guideline is copied from the description there (variable names are modified):\n",
    "\n",
    "If you are going to optimize your own objective function with DE, you may try the following classical settings for the input file first: Choose method e.g. DE/rand/1/bin, set the population size to 10 times the number of parameters, select weighting factor F=0.8, and crossover constant CR=0.9. \n",
    "It has been found recently that selecting F from the interval (0.5, 1.0) randomly for each generation or for each difference vector, a technique called dither, improves convergence behavior significantly, especially for noisy objective functions. \n",
    "\n",
    "\n",
    "It has also been found that setting CR to a low value, e.g. CR=0.2 helps optimizing separable functions since it fosters the search along the coordinate axes. On the contrary this choice is not effective if parameter dependence is encountered, something which is frequently occurring in real-world optimization problems rather than artificial test functions. So for parameter dependence the choice of CR=0.9 is more appropriate. Another interesting empirical finding is that raising NP above, say, 40 does not substantially improve the convergence, independent of the number of parameters. It is worthwhile to experiment with these suggestions. Make sure that you initialize your parameter vectors by exploiting their full numerical range, i.e. if a parameter is allowed to exhibit values in the range (-100, 100) it's a good idea to pick the initial values from this range instead of unnecessarily restricting diversity.\n",
    "\n",
    "\n",
    "Keep in mind that different problems often require different settings for NP, F and CR (have a look into the different papers to get a feeling for the settings). If you still get misconvergence you might want to try a different method. We mostly use 'DE/rand/1/...' or 'DE/best/1/...'. The crossover method is not so important although Ken Price claims that binomial is never worse than exponential. In case of misconvergence also check your choice of objective function. There might be a better one to describe your problem. Any knowledge that you have about the problem should be worked into the objective function. A good objective function can make all the difference.\n",
    "\n",
    "And this is how DE can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code": "algorithms/usage_de.py"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found: \n",
      "X = [-2.80577434e-07 -2.54561753e-07 -1.79775569e-07  1.29067927e-08\n",
      " -3.39158569e-07 -1.39999611e-07  1.60854773e-07 -3.21286978e-07\n",
      "  9.96888532e-07 -9.21603681e-07]\n",
      "F = [1.91119161e-06]\n"
     ]
    }
   ],
   "source": [
    "from pymoo.algorithms.so_de import DE\n",
    "from pymoo.factory import get_problem\n",
    "from pymoo.operators.sampling.latin_hypercube_sampling import LatinHypercubeSampling\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "\n",
    "problem = get_problem(\"ackley\", n_var=10)\n",
    "\n",
    "algorithm = DE(\n",
    "    pop_size=100,\n",
    "    sampling=LatinHypercubeSampling(iterations=100, criterion=\"maxmin\"),\n",
    "    variant=\"DE/rand/1/bin\",\n",
    "    CR=0.5,\n",
    "    F=0.3,\n",
    "    dither=\"vector\",\n",
    "    jitter=False\n",
    ")\n",
    "\n",
    "res = minimize(problem,\n",
    "               algorithm,\n",
    "               seed=1,\n",
    "               verbose=False)\n",
    "\n",
    "print(\"Best solution found: \\nX = %s\\nF = %s\" % (res.X, res.F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. autoclass:: pymoo.algorithms.so_de.DE\n",
    "    :noindex:"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
